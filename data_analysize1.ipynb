{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30a1f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "import json # 用于后面的流程\n",
    "\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "from pyspark.sql.types import StringType, IntegerType, StructField, StructType\n",
    "\n",
    "stopwords = [i.strip() for i in open('chineseStopWords.txt',encoding='utf-8').readlines()]\n",
    "def pretty_cut(sentence):\n",
    "    cut_list = jieba.lcut(''.join(re.findall('[\\u4e00-\\u9fa5]', sentence)), cut_all=False)\n",
    "    for i in range(len(cut_list) - 1, -1, -1):\n",
    "        if cut_list[i] in stopwords:\n",
    "            del cut_list[i]\n",
    "    return cut_list\n",
    "\n",
    "def initialize(txt_file):\n",
    "    # 创建SparkContext和SparkSession对象\n",
    "#     sc = SparkContext('local', 'spark_project')\n",
    "#     sc.setLogLevel('WARN')\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    # 把数据加载为RDD，并且对每一行数据进行切割，转为Row对象\n",
    "    rdd = spark.sparkContext.textFile(txt_file)\\\n",
    "        .map(lambda x: x.split(\"\\t\")).map(lambda x:Row(x[0],x[1],x[2],x[3],int(x[4]),int(x[5]),int(x[6]),\\\n",
    "                                                        int(x[7]),int(x[8]),int(x[9]),int(x[10]),x[11],x[12],int(x[13])))\n",
    "    # 定义数据集结构\n",
    "    fields = [StructField(\"up\", StringType(), False), StructField(\"time\", StringType(), False),\n",
    "              StructField(\"title\", StringType(), False),StructField(\"desc\", StringType(), False),\n",
    "              StructField(\"view\", IntegerType(), False), StructField(\"danmaku\", IntegerType(), False),\n",
    "              StructField(\"reply\", IntegerType(), False), StructField(\"favorite\", IntegerType(), False),\n",
    "              StructField(\"coin\", IntegerType(), False), StructField(\"share\", IntegerType(), False),\n",
    "              StructField(\"like\", IntegerType(), False), StructField(\"rcmd_reason\", StringType(), False),\n",
    "              StructField(\"tname\", StringType(), False), StructField(\"his_rank\", IntegerType(), False), ]\n",
    "    schema = StructType(fields)\n",
    "    # 将RDD转为Dataframe\n",
    "    data = spark.createDataFrame(rdd, schema)\n",
    "    # 注册sql临时视图\n",
    "    data.createOrReplaceTempView(\"data\")\n",
    "    return spark\n",
    "\n",
    "# 入选次数最多的10个up主\n",
    "def top_popular_up(spark,base_dir):\n",
    "    popular_up = spark.sql(\n",
    "        \"SELECT up,COUNT(up) AS popular_up_times FROM data GROUP BY up ORDER BY popular_up_times DESC LIMIT 10\")\n",
    "    data = popular_up.toPandas()\n",
    "    save_dir = os.path.join(base_dir,'top_popular_up.csv')\n",
    "    data.to_csv(save_dir,index=False)\n",
    "\n",
    "# 入选次数最多的10个主题\n",
    "def top_popular_subject(spark,base_dir):\n",
    "    popular_subject = spark.sql(\n",
    "        \"SELECT tname,COUNT(tname) AS popular_subject_times FROM data GROUP BY tname ORDER BY popular_subject_times DESC LIMIT 10\")\n",
    "    data = popular_subject.toPandas()\n",
    "    save_dir = os.path.join(base_dir,'top_popular_subject.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "# 播放量最多的10个视频与总播放量最多的10个up主\n",
    "def top_popular_view(spark,base_dir):\n",
    "    view_data = spark.sql(\n",
    "        \"SELECT title,view AS view_data FROM data ORDER BY view DESC LIMIT 10\")\n",
    "    data = view_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'video_view_data.csv')\n",
    "    data.to_csv(save_dir,index=False)\n",
    "\n",
    "    view_data = spark.sql(\n",
    "        \"SELECT up,sum(view) AS view_data FROM data GROUP BY up ORDER BY sum(view) DESC LIMIT 10\")\n",
    "    data = view_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'up_view_data.csv')\n",
    "    data.to_csv(save_dir,index=False)\n",
    "\n",
    "# 弹幕数量最多的10个视频与总弹幕数量最多的10个up主\n",
    "def top_popular_danmaku(spark,base_dir):\n",
    "    danmaku_data = spark.sql(\n",
    "        \"SELECT title,danmaku AS danmaku_data FROM data ORDER BY danmaku DESC LIMIT 10\")\n",
    "    data = danmaku_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'video_danmaku_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "    danmaku_data = spark.sql(\n",
    "        \"SELECT up,sum(danmaku) AS danmaku_data FROM data GROUP BY up ORDER BY sum(danmaku) DESC LIMIT 10\")\n",
    "    data = danmaku_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'up_danmaku_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "# 回复量最多的10个视频与总回复量最多的10个up主\n",
    "def top_popular_reply(spark,base_dir):\n",
    "    reply_data = spark.sql(\n",
    "        \"SELECT title,reply AS reply_data FROM data ORDER BY reply DESC LIMIT 10\")\n",
    "    data = reply_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'video_reply_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "    reply_data = spark.sql(\n",
    "        \"SELECT up,sum(reply) AS reply_data FROM data GROUP BY up ORDER BY sum(reply) DESC LIMIT 10\")\n",
    "    data = reply_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'up_reply_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "# 收藏次数最多的10个视频与总收藏次数最多的10个up主\n",
    "def top_popular_favorite(spark,base_dir):\n",
    "    favorite_data = spark.sql(\n",
    "        \"SELECT title,favorite AS favorite_data FROM data ORDER BY favorite DESC LIMIT 10\")\n",
    "    data = favorite_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'video_favorite_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "    favorite_data = spark.sql(\n",
    "        \"SELECT up,sum(favorite) AS favorite_data FROM data GROUP BY up ORDER BY sum(favorite) DESC LIMIT 10\")\n",
    "    data = favorite_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'up_favorite_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "# 投币数最多的10个视频与总投币数最多的10个up主\n",
    "def top_popular_coin(spark,base_dir):\n",
    "    coin_data = spark.sql(\n",
    "        \"SELECT title,coin AS coin_data FROM data ORDER BY coin DESC LIMIT 10\")\n",
    "    data = coin_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'video_coin_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "    coin_data = spark.sql(\n",
    "        \"SELECT up,sum(coin) AS coin_data FROM data GROUP BY up ORDER BY sum(coin) DESC LIMIT 10\")\n",
    "    data = coin_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'up_coin_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "# 分享次数最多的10个视频与总分享次数最多的10个up主\n",
    "def top_popular_share(spark,base_dir):\n",
    "    share_data = spark.sql(\n",
    "        \"SELECT title,share AS share_data FROM data ORDER BY share DESC LIMIT 10\")\n",
    "    data = share_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'video_share_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "    share_data = spark.sql(\n",
    "        \"SELECT up,sum(share) AS share_data FROM data GROUP BY up ORDER BY sum(share) DESC LIMIT 10\")\n",
    "    data = share_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'up_share_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "# 点赞数最多的10个视频与总点赞数最多的10个up主\n",
    "def top_popular_like(spark,base_dir):\n",
    "    like_data = spark.sql(\n",
    "        \"SELECT title,like AS like_data FROM data ORDER BY like DESC LIMIT 10\")\n",
    "    data = like_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir,'video_like_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "    like_data = spark.sql(\n",
    "        \"SELECT up,sum(like) AS like_data FROM data GROUP BY up ORDER BY sum(like) DESC LIMIT 10\")\n",
    "    data = like_data.toPandas()\n",
    "    save_dir = os.path.join(base_dir,'up_like_data.csv')\n",
    "    data.to_csv(save_dir, index=False)\n",
    "\n",
    "# 词频统计\n",
    "def word_count(spark, base_dir):\n",
    "    # 统计标题所有词的词频\n",
    "    wordCount_title = spark.sql(\"SELECT title as title from data\").rdd.flatMap(\n",
    "        lambda line: pretty_cut(line['title'])).map(lambda word: (word, 1)).reduceByKey(\n",
    "        lambda a, b: a + b).repartition(1).sortBy(lambda x: x[1], False)\n",
    "    # 转为Dataframe\n",
    "    wordCountSchema = StructType([StructField(\"word\", StringType(), True), StructField(\"count\", IntegerType(), True)])\n",
    "    wordCountDF = spark.createDataFrame(wordCount_title, wordCountSchema)\n",
    "    # 过滤掉空值\n",
    "    wordCountDF = wordCountDF.filter(wordCountDF[\"word\"] != '')\n",
    "    # 只保留频数前300的单词\n",
    "    wordCount_title_300 = wordCountDF.take(300)\n",
    "    wordCount_title_300 = spark.createDataFrame(wordCount_title_300, wordCountSchema).toPandas()\n",
    "    # 保存为csv文件\n",
    "    save_dir = os.path.join(base_dir, 'title_word.csv')\n",
    "    wordCount_title_300.to_csv(save_dir, index=False)\n",
    "\n",
    "    # 视频简介词频统计，操作同上\n",
    "    wordCount_desc = spark.sql(\"SELECT desc as description from data\").rdd.flatMap(\n",
    "        lambda line: pretty_cut(line['description'])).map(lambda word: (word, 1)).reduceByKey(\n",
    "        lambda a, b: a + b).repartition(1).sortBy(lambda x: x[1], False)\n",
    "    wordCountSchema = StructType([StructField(\"word\", StringType(), True), StructField(\"count\", IntegerType(), True)])\n",
    "    wordCountDF = spark.createDataFrame(wordCount_desc, wordCountSchema)\n",
    "    wordCountDF = wordCountDF.filter(wordCountDF[\"word\"] != '')\n",
    "    wordCount_desc_300 = wordCountDF.take(300)\n",
    "    wordCount_desc_300 = spark.createDataFrame(wordCount_desc_300, wordCountSchema).toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'desc_word.csv')\n",
    "    wordCount_desc_300.to_csv(save_dir, index=False)\n",
    "\n",
    "    # 视频简介词频统计，操作同上\n",
    "    wordCount_rcmd_reason = spark.sql(\"SELECT rcmd_reason as rcmd_reason from data\").rdd.flatMap(\n",
    "        lambda line: pretty_cut(line['rcmd_reason'])).map(lambda word: (word, 1)).reduceByKey(\n",
    "        lambda a, b: a + b).repartition(1).sortBy(lambda x: x[1], False)\n",
    "    wordCountSchema = StructType([StructField(\"word\", StringType(), True), StructField(\"count\", IntegerType(), True)])\n",
    "    wordCountDF = spark.createDataFrame(wordCount_rcmd_reason, wordCountSchema)\n",
    "    wordCountDF = wordCountDF.filter(wordCountDF[\"word\"] != '')\n",
    "    wordCount_rcmd_reason_300 = wordCountDF.take(300)\n",
    "    wordCount_rcmd_reason_300 = spark.createDataFrame(wordCount_rcmd_reason_300, wordCountSchema).toPandas()\n",
    "    save_dir = os.path.join(base_dir, 'rcmd_reason_word.csv')\n",
    "    wordCount_rcmd_reason_300.to_csv(save_dir, index=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # HDFS文件路径\n",
    "    txt_file = 'hdfs://master:8020/data/bilibili_week.txt'\n",
    "    # 初始化\n",
    "    spark = initialize(txt_file)\n",
    "    # 分析结果的存储路径\n",
    "    base_dir = 'static/'\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.makedirs(base_dir,exist_ok=True)\n",
    "    # 开始进行分析\n",
    "    # 分析视频入选最多的up主\n",
    "    top_popular_up(spark, base_dir)\n",
    "    # 分析视频入选最多的视频分区\n",
    "    top_popular_subject(spark, base_dir)\n",
    "    # 分析播放量\n",
    "    top_popular_view(spark,base_dir)\n",
    "    # 分析弹幕数\n",
    "    top_popular_danmaku(spark, base_dir)\n",
    "    # 分析评论数\n",
    "    top_popular_reply(spark, base_dir)\n",
    "    # 分析收藏数\n",
    "    top_popular_favorite(spark, base_dir)\n",
    "    # 分析投币数\n",
    "    top_popular_coin(spark, base_dir)\n",
    "    # 分析分享次数\n",
    "    top_popular_share(spark, base_dir)\n",
    "    # 分析点赞数量\n",
    "    top_popular_like(spark, base_dir)\n",
    "    # 分析词频\n",
    "    word_count(spark, base_dir)\n",
    "    print(\"finish!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
